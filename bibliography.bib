@article{goodfellow2015explaining,
  title={Explaining and Harnessing Adversarial Examples},
  author={Ian Goodfellow and Jonathon Shlens and Christian Szegedy},
  journal={},
  year={2018}
}

@article{amodei,
  title={{AI} and Compute},
  author={Amodei, Dario and Hernandez, Danny},
  journal={Downloaded from https://blog.openai.com/ai-and-compute},
  year={2018}
}


@inproceedings{li2014scaling,
  title={Scaling Distributed Machine Learning with the Parameter Server.},
  author={Li, Mu and Andersen, David G and Park, Jun Woo and Smola, Alexander J and Ahmed, Amr and Josifovski, Vanja and Long, James and Shekita, Eugene J and Su, Bor-Yiing},
  booktitle={OSDI},
  volume={1},
  pages={3},
  year={2014}
}


@inproceedings{chiu1993robustness,
  title={Robustness of feedforward neural networks},
  author={Chiu, Ching-Tai and others},
  booktitle={IEEE International Conference on Neural Networks},
  pages={783--788},
  year={1993},
  organization={IEEE}
}


@article{neti1992maximally,
  title={Maximally fault tolerant neural networks},
  author={Neti, Chalapathy and Schneider, Michael H and Young, Eric D},
  journal={IEEE Transactions on Neural Networks},
  volume={3},
  number={1},
  pages={14--23},
  year={1992},
  publisher={IEEE}
}

@techreport{darpa,
  title={Fault Tolerance of Neural Networks},
  author={Mehrotra, Kishan and Mohan, Chilukuri K and Ranka, Sanjay and Chiu, Ching-tai},
  year={1994},
  institution={DTIC Document}
}

@article{piuri,
  title={Analysis of fault tolerance in artificial neural networks},
  author={Piuri, Vincenzo},
  journal={Journal of Parallel and Distributed Computing},
  volume={61},
  number={1},
  pages={18--48},
  year={2001},
  publisher={Elsevier}
}



@inproceedings{bengio,
  title={Use of neural networks for the recognition of place of articulation},
  author={Bengio, Yoshua and De Mori, Renato},
  booktitle={Acoustics, Speech, and Signal Processing, 1988. ICASSP-88., 1988 International Conference on},
  pages={103--106},
  year={1988},
  organization={IEEE}
}



@book{connectionism,
  title={Parallel distributed processing},
  author={Rumelhart, David E and McClelland, James L and PDP Research Group and others},
  volume={1},
  year={1987},
  publisher={MIT press Cambridge, MA}
}


@article{perceptron,
  title={The perceptron: a probabilistic model for information storage and organization in the brain.},
  author={Rosenblatt, Frank},
  journal={Psychological review},
  volume={65},
  number={6},
  pages={386},
  year={1958},
  publisher={American Psychological Association}
}



@book{haykin2009neural,
  title={Neural networks and learning machines},
  author={Haykin, Simon},
  volume={3},
  year={2009},
  publisher={Pearson Education Upper Saddle River}
}

@article{navlakha2015distributed,
  title={Distributed information processing in biological and computational systems},
  author={Navlakha, Saket and Bar-Joseph, Ziv},
  journal={Communications of the ACM},
  volume={58},
  number={1},
  pages={94--102},
  year={2015},
  publisher={ACM}
}

@inproceedings{ghaffari2015distributed,
  title={Distributed house-hunting in ant colonies},
  author={Ghaffari, Mohsen and Musco, Cameron and Radeva, Tsvetomira and Lynch, Nancy},
  booktitle={Proceedings of the 2015 ACM Symposium on Principles of Distributed Computing},
  pages={57--66},
  year={2015},
  organization={ACM}
}

@inproceedings{ontherobustness17,
  title={On the robustness of a neural network},
  author = {El{ }Mahdi El{ }Mhamdi and Rachid Guerraoui and SÃ©bastien Rouault},
  booktitle={Reliable Distributed Systems (SRDS), 2017 IEEE 36th Symposium on},
  pages={84--93},
  year={2017},
  organization={IEEE}
}

@INPROCEEDINGS{whenneuronsfail17,
author={E. M. El Mhamdi and R. Guerraoui}, 
booktitle={2017 IEEE International Parallel and Distributed Processing Symposium (IPDPS)}, 
title={When Neurons Fail}, 
year={2017}, 
volume={}, 
number={}, 
pages={1028-1037}, 
keywords={combinatorial mathematics;learning (artificial intelligence);neural nets;learning phases;multilayer neural network;distributed system;neural activation functions;Lipschitz-continuous;forward error propagation;failure situations;combinatorial explosion;neural activation function;Byzantine failures;synaptic transmission capacity;memory cost reduction;neural network;boosting scheme;unnecessary signals;neural networks robustness;learning cost;Neurons;Biological neural networks;Robustness;Computer crashes;Mathematical model;Nonhomogeneous media;Computational modeling;Neural Networks;Neuromorphic computing;Byzantine Fault Tolerance;Robustness;Machine Learning;Distributed Systems}, 
doi={10.1109/IPDPS.2017.66}, 
ISSN={1530-2075}, 
month={May},}

@inproceedings{bartlett2017spectrally,
	title={Spectrally-normalized margin bounds for neural networks},
	author={Bartlett, Peter L and Foster, Dylan J and Telgarsky, Matus J},
	booktitle={Advances in Neural Information Processing Systems},
	pages={6240--6249},
	year={2017}
}

@article{yoshida2017spectral,
	title={Spectral Norm Regularization for Improving the Generalizability of Deep Learning},
	author={Yoshida, Yuichi and Miyato, Takeru},
	journal={arXiv preprint arXiv:1705.10941},
	year={2017}
}

@article{hammadi1997learning,
	title={A learning algorithm for fault tolerant feedforward neural networks},
	author={Hammadi, Nait Charif and Ito, Hideo},
	journal={IEICE TRANSACTIONS on Information and Systems},
	volume={80},
	number={1},
	pages={21--27},
	year={1997},
	publisher={The Institute of Electronics, Information and Communication Engineers}
}

@article{murray1994enhanced,
	title={Enhanced MLP performance and fault tolerance resulting from synaptic weight noise during training},
	author={Murray, Alan F and Edwards, Peter J},
	journal={IEEE Transactions on neural networks},
	volume={5},
	number={5},
	pages={792--802},
	year={1994},
	publisher={IEEE}
}

@article{finlay2018improved,
	title={Improved robustness to adversarial examples using Lipschitz regularization of the loss},
	author={Finlay, Chris and Oberman, Adam and Abbasi, Bilal},
	journal={arXiv preprint arXiv:1810.00953},
	year={2018}
}

@article{phatak1995complete,
	title={Complete and partial fault tolerance of feedforward neural nets},
	author={Phatak, Dhananjay S and Koren, Israel},
	journal={IEEE Transactions on Neural Networks},
	volume={6},
	number={2},
	pages={446--456},
	year={1995},
	publisher={IEEE}
}

@inproceedings{wang2016analysis,
  title={Analysis of deep neural networks with extended data Jacobian matrix},
  author={Wang, Shengjie and Mohamed, Abdel-rahman and Caruana, Rich and Bilmes, Jeff and Plilipose, Matthai and Richardson, Matthew and Geras, Krzysztof and Urban, Gregor and Aslan, Ozlem},
  booktitle={International Conference on Machine Learning},
  pages={718--726},
  year={2016}
}

@inproceedings{baldi2013understanding,
  title={Understanding dropout},
  author={Baldi, Pierre and Sadowski, Peter J},
  booktitle={Advances in neural information processing systems},
  pages={2814--2822},
  year={2013}
}

@phdthesis{kerlirzinThese,
  title={Etude de la robustesse des r{\'e}seaux multicouches},
  author={Kerlirzin, Philippe},
  year={1994},
  school={Paris 11}
}

@article{kerlirzin,
  title={Robustness in multilayer perceptrons},
  author={Kerlirzin, P and Vallet, F},
  journal={Neural computation},
  volume={5},
  number={3},
  pages={473--482},
  year={1993},
  publisher={MIT Press}
}

@inproceedings{alexnet,
  title={Imagenet classification with deep convolutional neural networks},
  author={Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
  booktitle={Advances in neural information processing systems},
  pages={1097--1105},
  year={2012}
}

@article{hintonDropout,
  title={Improving neural networks by preventing co-adaptation of feature detectors},
  author={Hinton, Geoffrey E and Srivastava, Nitish and Krizhevsky, Alex and Sutskever, Ilya and Salakhutdinov, Ruslan R},
  journal={arXiv preprint arXiv:1207.0580},
  year={2012}
}

@inproceedings{deepmindremoval,
title={On the importance of single directions for generalization},
author={Ari S. Morcos and David G.T. Barrett and Neil C. Rabinowitz and Matthew Botvinick},
booktitle={International Conference on Learning Representations},
year={2018},
url={https://openreview.net/forum?id=r1iuQjxCZ},
}

@inproceedings{NVIDIAerror,
  title={Understanding error propagation in deep learning neural network (DNN) accelerators and applications},
  author={Li, Guanpeng and Hari, Siva Kumar Sastry and Sullivan, Michael and Tsai, Timothy and Pattabiraman, Karthik and Emer, Joel and Keckler, Stephen W},
  booktitle={Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis},
  pages={8},
  year={2017},
  organization={ACM}
}


@inproceedings{poole2016exponential,
  title={Exponential expressivity in deep neural networks through transient chaos},
  author={Poole, Ben and Lahiri, Subhaneil and Raghu, Maithra and Sohl-Dickstein, Jascha and Ganguli, Surya},
  booktitle={Advances in neural information processing systems},
  pages={3360--3368},
  year={2016}
}

@inproceedings{raghu2017expressive,
  title={On the Expressive Power of Deep Neural Networks},
  author={Raghu, Maithra and Poole, Ben and Kleinberg, Jon and Ganguli, Surya and Sohl-Dickstein, Jascha},
  booktitle={International Conference on Machine Learning},
  pages={2847--2854},
  year={2017}
}

@book{bovier2012mathematical,
  title={Mathematical aspects of spin glasses and neural networks},
  author={Bovier, Anton and Picco, Pierre},
  volume={41},
  year={2012},
  publisher={Springer Science \& Business Media}
}

@inproceedings{choromanska2015open,
  title={Open problem: The landscape of the loss surfaces of multilayer networks},
  author={Choromanska, Anna and LeCun, Yann and Arous, G{\'e}rard Ben},
  booktitle={Conference on Learning Theory},
  pages={1756--1760},
  year={2015}
}

@inproceedings{lecun1990optimal,
  title={Optimal brain damage},
  author={LeCun, Yann and Denker, John S and Solla, Sara A},
  booktitle={Advances in neural information processing systems},
  pages={598--605},
  year={1990}
}

@article{liu2015gene,
  title={Gene essentiality is a quantitative property linked to cellular evolvability},
  author={Liu, Gaowen and Yong, Mei Yun Jacy and Yurieva, Marina and Srinivasan, Kandhadayar Gopalan and Liu, Jaron and Lim, John Soon Yew and Poidinger, Michael and Wright, Graham Daniel and others},
  journal={Cell},
  volume={163},
  number={6},
  pages={1388--1399},
  year={2015},
  publisher={Elsevier}
}

@article{fraser2015essential,
  title={Essential human genes},
  author={Fraser, Andrew},
  journal={Cell systems},
  volume={1},
  number={6},
  pages={381--382},
  year={2015},
  publisher={Elsevier}
}

@article{lecun2015deep,
  title={Deep learning},
  author={LeCun, Yann and Bengio, Yoshua and Hinton, Geoffrey},
  journal={Nature},
  volume={521},
  number={7553},
  pages={436--444},
  year={2015},
  publisher={Nature Research}
}

@article{silver2016mastering,
  title={Mastering the game of Go with deep neural networks and tree search},
  author={Silver, David and Huang, Aja and Maddison, Chris J and Guez, Arthur and Sifre, Laurent and Van Den Driessche, George and Schrittwieser, Julian and Antonoglou, Ioannis and Panneershelvam, Veda and Lanctot, Marc and others},
  journal={nature},
  volume={529},
  number={7587},
  pages={484},
  year={2016},
  publisher={Nature Publishing Group}
}

@inproceedings{krizhevsky2012imagenet,
  title={Imagenet classification with deep convolutional neural networks},
  author={Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
  booktitle={Advances in neural information processing systems},
  pages={1097--1105},
  year={2012}
}

@article{julie,
  title={Vowel recognition with four coupled spin-torque nano-oscillators},
  author={Romera, Miguel and Talatchian, Philippe and Tsunegi, Sumito and Araujo, Flavio Abreu and Cros, Vincent and Bortolotti, Paolo and Trastoy, Juan and Yakushiji, Kay and Fukushima, Akio and Kubota, Hitoshi and others},
  journal={Nature},
  volume={563},
  number={7730},
  pages={230},
  year={2018},
  publisher={Nature Publishing Group}
}


@inproceedings{neuromorphicIBM,
  title={Backpropagation for energy-efficient neuromorphic computing},
  author={Esser, Steve K and Appuswamy, Rathinakumar and Merolla, Paul and Arthur, John V and Modha, Dharmendra S},
  booktitle={Advances in Neural Information Processing Systems},
  pages={1117--1125},
  year={2015}
}

@inproceedings{biggio2013evasion,
  title={Evasion attacks against machine learning at test time},
  author={Biggio, Battista and Corona, Igino and Maiorca, Davide and Nelson, Blaine and {\v{S}}rndi{\'c}, Nedim and Laskov, Pavel and Giacinto, Giorgio and Roli, Fabio},
  booktitle={Joint European conference on machine learning and knowledge discovery in databases},
  pages={387--402},
  year={2013},
  organization={Springer}
}

@inproceedings{moosavi2017universal,
  title={Universal Adversarial Perturbations},
  author={Moosavi-Dezfooli, Seyed-Mohsen and Fawzi, Alhussein and Fawzi, Omar and Frossard, Pascal},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  pages={1765--1773},
  year={2017}
}
@Article{Bernier2000,
author="Bernier, Jose L.
and Ortega, J.
and Rojas, I.
and Ros, E.
and Prieto, A.",
title="Obtaining Fault Tolerant Multilayer Perceptrons Using an Explicit Regularization",
journal="Neural Processing Letters",
year="2000",
month="Oct",
day="01",
volume="12",
number="2",                                                                                                                                                                       
pages="107--113",                                                                                                                                                                 
abstract="When the learning algorithm is applied to a MLP structure, different solutions for the weight values can be obtained if the parameters of the applied rule or the initial conditions are changed. Those solutions can present similar performance with respect to learning, but they differ in other aspects, in particular, fault tolerance against weight perturbations. In this paper, a backpropagation algorithm that maximizes fault tolerance is proposed. The algorithm presented explicitly adds a new term to the backpropagation learning rule related to the mean square error degradation in the presence of weight deviations in order to minimize this degradation. The results obtained demonstrate the efficiency of the learning rule proposed here in comparison with other algorithm.",                                                                                                     
issn="1573-773X",                                                                                                                                                                 
doi="10.1023/A:1009698206772",                                                                                                                                                    
url="https://doi.org/10.1023/A:1009698206772"                                                                                                                                     
}                                                                                                                                                                                 

@inproceedings{szegedy2013intriguing,
title	= {Intriguing properties of neural networks},
author	= {Christian Szegedy and Wojciech Zaremba and Ilya Sutskever and Joan Bruna and Dumitru Erhan and Ian Goodfellow and Rob Fergus},
year	= {2014},
URL	= {http://arxiv.org/abs/1312.6199},
booktitle	= {International Conference on Learning Representations}
}

@article{gouk2018regularisation,
	title={Regularisation of Neural Networks by Enforcing Lipschitz Continuity},
	author={Gouk, Henry and Frank, Eibe and Pfahringer, Bernhard and Cree, Michael},
	journal={arXiv preprint arXiv:1804.04368},
	year={2018}
}

@inproceedings{arechiga2018robustness,
  title={The Robustness of Modern Deep Learning Architectures against Single Event Upset Errors},
  author={Arechiga, Austin P and Michaels, Alan J},
  booktitle={2018 IEEE High Performance extreme Computing Conference (HPEC)},
  pages={1--6},
  year={2018},
  organization={IEEE}
}

@inproceedings{parseval,
  title={Parseval Networks: Improving Robustness to Adversarial Examples},
  author={Cisse, Moustapha and Bojanowski, Piotr and Grave, Edouard and Dauphin, Yann and Usunier, Nicolas},
  booktitle={International Conference on Machine Learning},
  pages={854--863},
  year={2017}
}

@InProceedings{bulyanPaper,
  author    = {El{ }Mhamdi, El{ }Mahdi and Guerraoui, Rachid and Rouault, S{\'e}bastien},
  title     = {The Hidden Vulnerability of Distributed Learning in {B}yzantium},
  booktitle = {Proceedings of the 35th International Conference on Machine Learning},
  year      = {2018},
  volume    = {80},
  series    = {Proceedings of Machine Learning Research},
  pages     = {3521--3530},
  address   = {StockholmsmÃ¤ssan, Stockholm Sweden},
  month     = {10--15 Jul},
  publisher = {PMLR},
}

@article{galloway2018adversarial,
  title={Adversarial Examples as an Input-Fault Tolerance Problem},
  author={Galloway, Angus and Golubeva, Anna and Taylor, Graham W},
  journal={arXiv preprint arXiv:1811.12601},
  year={2018}
}

@article{helmbold2017surprising,
  title={Surprising properties of dropout in deep networks},
  author={Helmbold, David P and Long, Philip M},
  journal={The Journal of Machine Learning Research},
  volume={18},
  number={1},
  pages={7284--7311},
  year={2017},
  publisher={JMLR.org}
}

@inproceedings{park2018adversarial,
  title={Adversarial dropout for supervised and semi-supervised learning},
  author={Park, Sungrae and Park, JunKeon and Shin, Su-Jin and Moon, Il-Chul},
  booktitle={Thirty-Second AAAI Conference on Artificial Intelligence},
  year={2018}
}

@incollection{jacot2018neural,
title = {Neural Tangent Kernel: Convergence and Generalization in Neural Networks},
author = {Jacot, Arthur and Gabriel, Franck and Hongler, Clement},
booktitle = {Advances in Neural Information Processing Systems 31},
editor = {S. Bengio and H. Wallach and H. Larochelle and K. Grauman and N. Cesa-Bianchi and R. Garnett},
pages = {8580--8589},
year = {2018},
publisher = {Curran Associates, Inc.},
url = {http://papers.nips.cc/paper/8076-neural-tangent-kernel-convergence-and-generalization-in-neural-networks.pdf}
}

@inproceedings{phatak1999relationship,
  title={Relationship between fault tolerance, generalization and the Vapnik-Chervonenkis (VC) dimension of feedforward ANNs},
  author={Phatak, Dhananjay S},
  booktitle={Neural Networks, 1999. IJCNN'99. International Joint Conference on},
  volume={1},
  pages={705--709},
  year={1999},
  organization={IEEE}
}

@article{amodei2016concrete,
  title={Concrete problems in AI safety},
  author={Amodei, Dario and Olah, Chris and Steinhardt, Jacob and Christiano, Paul and Schulman, John and Man{\'e}, Dan},
  journal={arXiv preprint arXiv:1606.06565},
  year={2016}
}

@inproceedings{goodfellow2014generative,
  title={Generative adversarial nets},
  author={Goodfellow, Ian and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
  booktitle={Advances in neural information processing systems},
  pages={2672--2680},
  year={2014}
}

@article{pennington2018emergence,
  title={The emergence of spectral universality in deep networks},
  author={Pennington, Jeffrey and Schoenholz, Samuel S and Ganguli, Surya},
  journal={arXiv preprint arXiv:1802.09979},
  year={2018}
}

@article{novak2018sensitivity,
  title={Sensitivity and generalization in neural networks: an empirical study},
  author={Novak, Roman and Bahri, Yasaman and Abolafia, Daniel A and Pennington, Jeffrey and Sohl-Dickstein, Jascha},
  journal={arXiv preprint arXiv:1802.08760},
  year={2018}
}

@article{mhamdi2019fatal,
  title={Fatal Brain Damage},
  author={Mhamdi, El Mahdi El and Guerraoui, Rachid and Volodin, Sergei},
  journal={arXiv preprint arXiv:1902.01686},
  year={2019}
}

@article{kumar2018general,
  title={A general metric for identifying adversarial images},
  author={Kumar, Siddharth Krishna},
  journal={arXiv preprint arXiv:1807.10335},
  year={2018}
}

@article{raghunathan2018certified,
  title={Certified defenses against adversarial examples},
  author={Raghunathan, Aditi and Steinhardt, Jacob and Liang, Percy},
  journal={arXiv preprint arXiv:1801.09344},
  year={2018}
}

@article{simon2018adversarial,
  title={Adversarial vulnerability of neural networks increases with input dimension},
  author={Simon-Gabriel, Carl-Johann and Ollivier, Yann and Bottou, L{\'e}on and Sch{\"o}lkopf, Bernhard and Lopez-Paz, David},
  journal={arXiv preprint arXiv:1802.01421},
  year={2018}
}


@article{zou2018lipschitz,
  title={On Lipschitz Bounds of General Convolutional Neural Networks},
  author={Zou, Dongmian and Balan, Radu and Singh, Maneesh},
  journal={arXiv preprint arXiv:1808.01415},
  year={2018}
}

@article{BehnamNeyshaburSrinadhBhojanapalli2018,
  title={A pac-bayesian approach to spectrally-normalized margin bounds for neural networks},
  author={Neyshabur, Behnam and Bhojanapalli, Srinadh and Srebro, Nathan},
  journal={arXiv preprint arXiv:1707.09564},
  year={2017}
}

@article{Neyshabur2017,
abstract = {With a goal of understanding what drives generalization in deep networks, we consider several recently suggested explanations, including norm-based control, sharpness and robustness. We study how these measures can ensure generalization, highlighting the importance of scale normalization, and making a connection between sharpness and PAC-Bayes theory. We then investigate how well the measures explain different observed phenomena.},
annote = {Lemma 1 avoids spectral bound},
archivePrefix = {arXiv},
arxivId = {1706.08947},
author = {Neyshabur, Behnam and Bhojanapalli, Srinadh and McAllester, David and Srebro, Nathan},
eprint = {1706.08947},
journal={coRR},
file = {:home/sergei/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Neyshabur et al. - 2017 - Exploring Generalization in Deep Learning.pdf:pdf},
title = {{Exploring Generalization in Deep Learning}},
url = {http://arxiv.org/abs/1706.08947},
year = {2017}
}
@article{Nagarajan2019,
annote = {Lemma E.1 avoids spectral bound},
author = {Nagarajan, Vaishnavh},
file = {:home/sergei/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Nagarajan - 2019 - Deterministic PAC-Bayesian generalization bounds for deep networks via generalizing noise-resilience.pdf:pdf},
pages = {1--36},
title = {{Deterministic PAC-Bayesian generalization bounds for deep networks via generalizing noise-resilience}},
year = {2019},
journal={coRR},
}

@article{rusiecki2005fault,
  title={Fault tolerant feedforward neural network with median neuron input function},
  author={Rusiecki, AL},
  journal={Electronics Letters},
  volume={41},
  number={10},
  pages={603--605},
  year={2005},
  publisher={IET}
}

@book{dooley2001designing,
  title={Designing Large Scale Lans: Help for Network Designers},
  author={Dooley, Kevin},
  year={2001},
  publisher={" O'Reilly Media, Inc."}
}

@article{torres2017fault,
  title={Fault and error tolerance in neural networks: A review},
  author={Torres-Huitzil, Cesar and Girau, Bernard},
  journal={IEEE Access},
  volume={5},
  pages={17322--17341},
  year={2017},
  publisher={IEEE}
}

@INPROCEEDINGS{548899, 
author={ {Naihong Wei} and {Shiyuan Yang} and {Shibai Tong}}, 
booktitle={Proceedings of International Conference on Neural Networks (ICNN'96)}, 
title={A modified learning algorithm for improving the fault tolerance of BP networks}, 
year={1996}, 
volume={1}, 
number={}, 
pages={247-252 vol.1}, 
keywords={neural nets;modified learning algorithm;fault-tolerant neural networks;back-propagation;BP networks;classification;internal hardware failures;Fault tolerance;Neural networks;Hardware;Automation;Learning systems;Redundancy;Pattern recognition;Parallel processing;Constraint optimization;Animation}, 
doi={10.1109/ICNN.1996.548899}, 
ISSN={}, 
month={June},}







@article{doi:10.1162/089976600300014782,
author = {Bernier, J. L. and Ortega, J. and Ros, E. and Rojas, I. and Prieto, A.},
title = {A Quantitative Study of Fault Tolerance, Noise Immunity, and Generalization Ability of MLPs},
journal = {Neural Computation},
volume = {12},
number = {12},
pages = {2941-2964},
year = {2000},
doi = {10.1162/089976600300014782},

URL = { 
        https://doi.org/10.1162/089976600300014782
    
},
eprint = { 
        https://doi.org/10.1162/089976600300014782
    
}
}

@article{kalra2016many,
  title={How many miles of driving would it take to demonstrate autonomous vehicle reliability},
  author={Kalra, N and Paddock, SM},
  journal={Driving to Safety},
  year={2016}
}

@article{benjamini1999noise,
  title={Noise sensitivity of Boolean functions and applications to percolation},
  author={Benjamini, Itai and Kalai, Gil and Schramm, Oded},
  journal={Publications Math{\'e}matiques de l'Institut des Hautes Etudes Scientifiques},
  volume={90},
  number={1},
  pages={5--43},
  year={1999},
  publisher={Springer}
}

@article{ghorbani2019investigation,
  title={An Investigation into Neural Net Optimization via Hessian Eigenvalue Density},
  author={Ghorbani, Behrooz and Krishnan, Shankar and Xiao, Ying},
  journal={arXiv preprint arXiv:1901.10159},
  year={2019}
}

@article{keskar2016large,
  title={On large-batch training for deep learning: Generalization gap and sharp minima},
  author={Keskar, Nitish Shirish and Mudigere, Dheevatsa and Nocedal, Jorge and Smelyanskiy, Mikhail and Tang, Ping Tak Peter},
  journal={arXiv preprint arXiv:1609.04836},
  year={2016}
}

@Misc{taylor_cross_validated,
  author  = {agronskiy},
  title   = {Taking the expectation of Taylor series (especially the remainder). https://stats.stackexchange.com/questions/70490/taking-the-expectation-of-taylor-series-especially-the-remainder},
  year    = {2013},
  journal = {Cross Validated},
  url     = {https://stats.stackexchange.com/questions/70490/taking-the-expectation-of-taylor-series-especially-the-remainder},
}

@article{schuman2017survey,
  title={A survey of neuromorphic computing and neural networks in hardware},
  author={Schuman, Catherine D and Potok, Thomas E and Patton, Robert M and Birdwell, J Douglas and Dean, Mark E and Rose, Garrett S and Plank, James S},
  journal={arXiv preprint arXiv:1705.06963},
  year={2017}
}

@article{niemiro2009fixed,
  title={Fixed precision MCMC estimation by median of products of averages},
  author={Niemiro, Wojciech and Pokarowski, Piotr},
  journal={Journal of Applied Probability},
  volume={46},
  number={2},
  pages={309--329},
  year={2009},
  publisher={Cambridge University Press}
}

@article{arratia1989tutorial,
  title={Tutorial on large deviations for the binomial distribution},
  author={Arratia, Richard and Gordon, Louis},
  journal={Bulletin of mathematical biology},
  volume={51},
  number={1},
  pages={125--131},
  year={1989},
  publisher={Springer}
}

@article{arora2018convergence,
	title={A convergence analysis of gradient descent for deep linear neural networks},
	author={Arora, Sanjeev and Cohen, Nadav and Golowich, Noah and Hu, Wei},
	journal={arXiv preprint arXiv:1810.02281},
	year={2018}
}

@article{song2018mean,
  title={A mean field view of the landscape of two-layers neural networks},
  author={Song, Mei and Montanari, Andrea and Nguyen, P},
  journal={Proceedings of the National Academy of Sciences},
  volume={115},
  pages={E7665--E7671},
  year={2018}
}

@article{lee2019wide,
  title={Wide neural networks of any depth evolve as linear models under gradient descent},
  author={Lee, Jaehoon and Xiao, Lechao and Schoenholz, Samuel S and Bahri, Yasaman and Sohl-Dickstein, Jascha and Pennington, Jeffrey},
  journal={arXiv preprint arXiv:1902.06720},
  year={2019}
}

@inproceedings{le2007continuous,
  title={Continuous neural networks},
  author={Le Roux, Nicolas and Bengio, Yoshua},
  booktitle={Artificial Intelligence and Statistics},
  pages={404--411},
  year={2007}
}

@inproceedings{tang2013learning,
  title={Learning stochastic feedforward neural networks},
  author={Tang, Yichuan and Salakhutdinov, Ruslan R},
  booktitle={Advances in Neural Information Processing Systems},
  pages={530--538},
  year={2013}
}

@article{guss2016deep,
  title={Deep function machines: Generalized neural networks for topological layer expression},
  author={Guss, William H},
  journal={arXiv preprint arXiv:1612.04799},
  year={2016}
}

@inproceedings{sonoda2017double,
  title={Double continuum limit of deep neural networks},
  author={Sonoda, Sho and Murata, Noboru},
  booktitle={ICML Workshop Principled Approaches to Deep Learning},
  year={2017}
}

@article{stilgoe2018machine,
  title={Machine learning, social learning and the governance of self-driving cars},
  author={Stilgoe, Jack},
  journal={Social studies of science},
  volume={48},
  number={1},
  pages={25--56},
  year={2018},
  publisher={SAGE Publications Sage UK: London, England}
}

@article{schwarting2018planning,
  title={Planning and decision-making for autonomous vehicles},
  author={Schwarting, Wilko and Alonso-Mora, Javier and Rus, Daniela},
  journal={Annual Review of Control, Robotics, and Autonomous Systems},
  year={2018},
  publisher={Annual Reviews}
}

@inproceedings{tran2011design,
  title={Design of neuromorphic logic networks and fault-tolerant computing},
  author={Tran, AH and Yanushkevich, SN and Lyshevski, SE and Shmerko, VP},
  booktitle={2011 11th IEEE International Conference on Nanotechnology},
  pages={457--462},
  year={2011},
  organization={IEEE}
}

@inproceedings{liu2019fault,
  title={Fault tolerance in neuromorphic computing systems},
  author={Liu, Mengyun and Xia, Lixue and Wang, Yu and Chakrabarty, Krishnendu},
  booktitle={Proceedings of the 24th Asia and South Pacific Design Automation Conference},
  pages={216--223},
  year={2019},
  organization={ACM}
}

@inproceedings{dinh2017sharp,
  title={Sharp minima can generalize for deep nets},
  author={Dinh, Laurent and Pascanu, Razvan and Bengio, Samy and Bengio, Yoshua},
  booktitle={Proceedings of the 34th International Conference on Machine Learning-Volume 70},
  pages={1019--1028},
  year={2017},
  organization={JMLR. org}
}

@article{weng2018proven,
  title={PROVEN: Certifying Robustness of Neural Networks with a Probabilistic Approach},
  author={Weng, Tsui-Wei and Chen, Pin-Yu and Nguyen, Lam M and Squillante, Mark S and Oseledets, Ivan and Daniel, Luca},
  journal={arXiv preprint arXiv:1812.08329},
  year={2018}
}

@article{chizat2019lazy,
	title={On Lazy Training in Differentiable Programming},
	author={Chizat, Lenaic and Oyallon, Edouard and Bach, Francis},
	year={2019}
}

@article{ilyas2019adversarial,
  title={Adversarial examples are not bugs, they are features},
  author={Ilyas, Andrew and Santurkar, Shibani and Tsipras, Dimitris and Engstrom, Logan and Tran, Brandon and Madry, Aleksander},
  journal={arXiv preprint arXiv:1905.02175},
  year={2019}
}

@article{doerig2019unfolding,
  title={The unfolding argument: Why IIT and other causal structure theories cannot explain consciousness},
  author={Doerig, Adrien and Schurger, Aaron and Hess, Kathryn and Herzog, Michael H},
  journal={Consciousness and cognition},
  volume={72},
  pages={49--59},
  year={2019},
  publisher={Elsevier}
}

@inproceedings{zeiler2014visualizing,
  title={Visualizing and understanding convolutional networks},
  author={Zeiler, Matthew D and Fergus, Rob},
  booktitle={European conference on computer vision},
  pages={818--833},
  year={2014},
  organization={Springer}
}

@misc{wiki_binomial,
  title = {Wikipedia. Binomial distribution. Section Tail Bounds},
  howpublished = {\url{https://en.wikipedia.org/wiki/Binomial_distribution}},
  note = {Accessed: 2019-09-19}
}

@article{esteva2017dermatologist,
  title={Dermatologist-level classification of skin cancer with deep neural networks},
  author={Esteva, Andre and Kuprel, Brett and Novoa, Roberto A and Ko, Justin and Swetter, Susan M and Blau, Helen M and Thrun, Sebastian},
  journal={Nature},
  volume={542},
  number={7639},
  pages={115},
  year={2017},
  publisher={Nature Publishing Group}
}

@article{costanzo2016global,
  title={A global genetic interaction network maps a wiring diagram of cellular function},
  author={Costanzo, Michael and VanderSluis, Benjamin and Koch, Elizabeth N and Baryshnikova, Anastasia and Pons, Carles and Tan, Guihong and Wang, Wen and Usaj, Matej and Hanchard, Julia and Lee, Susan D and others},
  journal={Science},
  volume={353},
  number={6306},
  pages={aaf1420},
  year={2016},
  publisher={American Association for the Advancement of Science}
}

@article{glass2006essential,
  title={Essential genes of a minimal bacterium},
  author={Glass, John I and Assad-Garcia, Nacyra and Alperovich, Nina and Yooseph, Shibu and Lewis, Matthew R and Maruf, Mahir and Hutchison, Clyde A and Smith, Hamilton O and Venter, J Craig},
  journal={Proceedings of the National Academy of Sciences},
  volume={103},
  number={2},
  pages={425--430},
  year={2006},
  publisher={National Acad Sciences}
}

@article{simonyan2014very,
  title={Very deep convolutional networks for large-scale image recognition},
  author={Simonyan, Karen and Zisserman, Andrew},
  journal={arXiv preprint arXiv:1409.1556},
  year={2014}
}
